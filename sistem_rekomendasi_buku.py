# -*- coding: utf-8 -*-
"""Sistem Rekomendasi Buku.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IQ8CDOY23Bn2nme-FGn1fO5PeXZZSWde

# **Dataset goodbooks-10k dari Kaggle**

## **Persiapan dan Import Library**

Library dasar yang dipakai untuk manipulasi data (pandas, numpy), visualisasi (seaborn, matplotlib), dan pengolahan teks (TF-IDF, cosine similarity).
"""

# Install Kaggle API untuk mengunduh dataset dari Kaggle
!pip install kaggle

# === Import Library yang Dibutuhkan ===

# General-purpose libraries
import os
import numpy as np
import pandas as pd

# Visualisasi
import matplotlib.pyplot as plt
import seaborn as sns

# Machine Learning & Preprocessing
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# TensorFlow / Deep Learning
import tensorflow as tf
from tensorflow import keras
from keras import layers

"""### Setup Kaggle API

Memasukkan kredensial Kaggle secara langsung di environment agar bisa akses dataset Kaggle.
"""

# Set environment variables untuk Kaggle API
os.environ['KAGGLE_USERNAME'] = "evameivinadwiana"
os.environ['KAGGLE_KEY'] = "efbafcb258eb3a5f3033f21f925e06ef"

"""### Download dan Ekstrak Dataset"""

# Download dataset goodbooks-10k dari Kaggle
!kaggle datasets download zygmunt/goodbooks-10k

# Extract file ZIP ke folder ./goodbooks10k (overwrite jika sudah ada)
!unzip -o goodbooks-10k.zip -d ./goodbooks10k

"""# **Data Understanding**

Ini tahap awal untuk memahami data secara keseluruhan, jenis data, ukuran dataset, dan struktur kolom.

### Load Dataset
"""

# Load dataset goodbooks-10k
books = pd.read_csv('./goodbooks10k/books.csv')
book_tags = pd.read_csv('./goodbooks10k/book_tags.csv')
ratings = pd.read_csv('./goodbooks10k/ratings.csv')
tags = pd.read_csv('./goodbooks10k/tags.csv')
to_read = pd.read_csv('./goodbooks10k/to_read.csv')

"""#### Mengecek Kolom dan Ukuran Dataset

Memeriksa jumlah entri di setiap dataset untuk mengetahui skala dan cakupan data. Juga memastikan kolom yang akan dipakai sudah benar.
"""

# Tampilkan kolom dari book_tags untuk memastikan nama kolom yang benar
print(book_tags.columns)

# Informasi ringkas jumlah data di setiap dataset
print('Jumlah buku:', len(books))
print('Jumlah tag buku:', len(book_tags['goodreads_book_id'].unique())) # Assuming 'goodreads_book_id' is the correct column name based on common Kaggle datasets. Adjust if necessary.
print('Jumlah penilaian pengguna:', len(ratings))
print('Jumlah pengguna yang memberikan rating:', len(ratings.user_id.unique()))
print('Jumlah tag yang tersedia:', len(tags))
print('Jumlah daftar buku yang akan dibaca:', len(to_read))

"""#### Preview Contoh Data

Melihat sekilas bentuk data, jenis kolom, dan nilai data pada dataset utama.
"""

# Menampilkan beberapa baris awal dari dataset books dan ratings
print('\nContoh data buku:')
print(books.head())

print('\nContoh data rating:')
print(ratings.head())

"""### Informasi Detail dan Statistik Deskriptif

*info()* untuk mengetahui tipe data dan jumlah data null (jika ada).*describe()* memberikan ringkasan statistik seperti mean, std, min, max untuk kolom numerik. Ini membantu kita memahami distribusi dan karakteristik data.
"""

print("===== BOOKS =====")
books.info()
print("\nDeskripsi Statistik:")
print(books.describe())
print("\nContoh Data:")
print(books.head())

print("\n===== BOOK_TAGS =====")
book_tags.info()
print("\nDeskripsi Statistik:")
print(book_tags.describe())
print("\nContoh Data:")
print(book_tags.head())

print("\n===== RATINGS =====")
ratings.info()
print("\nDeskripsi Statistik:")
print(ratings.describe())
print("\nContoh Data:")
print(ratings.head())

print("\n===== TAGS =====")
tags.info()
print("\nDeskripsi Statistik:")
print(tags.describe())
print("\nContoh Data:")
print(tags.head())

print("\n===== TO_READ =====")
to_read.info()
print("\nDeskripsi Statistik:")
print(to_read.describe())
print("\nContoh Data:")
print(to_read.head())

"""### Data Summary Tambahan

Mengetahui jumlah unik item seperti buku yang diberi rating, penulis, dan jenis tag membantu kita dalam analisis lebih lanjut.

"""

# Ringkasan tambahan untuk insight jumlah unik dan contoh data
print('Jumlah buku yang mendapatkan rating: ', len(ratings.book_id.unique()))
print('Jumlah penulis unik: ', books.authors.nunique())
print('Contoh penulis: ', books.authors.unique()[:10])

print("Informasi variabel tags (jenis kategori buku):")
tags.info()

print("\nContoh data tag:")
print(tags.head())

print("\nJumlah tag unik yang tersedia:", tags['tag_name'].nunique())

print('Banyak tipe tag (kategori buku):', tags['tag_name'].nunique())
print('Tipe tag buku (kategori):')
print(tags['tag_name'].unique()[:50])

"""## **Univariate Exploratory Data Analysis**

Tahap ini berfokus pada analisis satu variabel (univariate) untuk memahami distribusinya.

#### Cek Ukuran Dataset Ratings dan Books
"""

# Ukuran dataset ratings dan books
print(ratings.shape)
print(books.shape)

"""#### Preview Data Awal

"""

# Preview beberapa baris pertama
ratings.head()
books.head()

"""#### Statistik Deskriptif Ratings dan Books

Statistik deskriptif dan summary ini membantu memahami sebaran rating, jumlah pengguna aktif, dan buku yang dinilai.
"""

ratings.describe()
books.describe()

"""#### Summary User dan Buku di Ratings"""

print('Jumlah user_id (pengguna): ', len(ratings.user_id.unique()))
print('Jumlah book_id (buku): ', len(ratings.book_id.unique()))
print('Jumlah data rating: ', len(ratings))

"""## **Data Preprocessing**

Menampilkan ukuran (baris dan kolom) dari dataset *books, to_read, *dan* ratings.* Ini adalah langkah awal untuk memahami data.
"""

# Menampilkan ukuran dataset utama
print("books shape:", books.shape)
print("to_read shape:", to_read.shape)
print("ratings shape:", ratings.shape)

"""Menggabungkan semua book_id unik dari beberapa dataset berbeda, lalu mengurutkan dan menghilangkan duplikat supaya tahu jumlah buku unik keseluruhan di semua data.

"""

# Menggabungkan seluruh book_id pada kategori Buku
books_all = np.concatenate((
    books.book_id.unique(),
    ratings.book_id.unique(),
    book_tags.goodreads_book_id.unique(),
    to_read.book_id.unique()
))

# Mengurutkan dan menghapus duplikat
books_all = np.sort(np.unique(books_all))
print('Jumlah seluruh buku unik berdasarkan book_id:', len(books_all))

"""Menggabungkan semua *user_id* unik dari data rating (karena *users* tidak ada), lalu mengurutkan dan menghilangkan duplikat untuk mengetahui jumlah pengguna unik."""

# Menggabungkan seluruh user_id pada kategori Pengguna
user_all = np.concatenate((
    ratings.user_id.unique(),
))

# Menghapus duplikat dan mengurutkan
user_all = np.sort(np.unique(user_all))
print('Jumlah seluruh pengguna unik berdasarkan user_id:', len(user_all))

"""Menyesuaikan tipe data *book_id* menjadi integer di semua dataset supaya bisa digabung (merge) tanpa error tipe data."""

# Ubah tipe data book_id menjadi integer agar konsisten dan siap digabungkan
books['book_id'] = books['book_id'].astype(int)
book_tags_renamed = book_tags.rename(columns={'goodreads_book_id': 'book_id'})
book_tags_renamed['book_id'] = book_tags_renamed['book_id'].astype(int)
to_read['book_id'] = to_read['book_id'].astype(int)
ratings['book_id'] = ratings['book_id'].astype(int)

"""Menggabungkan data rating dengan data buku agar setiap rating punya informasi buku terkait."""

# Gabungkan data ratings dengan metadata buku berdasarkan book_id
books_ratings = pd.merge(ratings, books, on='book_id', how='left')
print(f"books_ratings shape: {books_ratings.shape}")

"""Menggabungkan tag buku ke data rating+book agar data lengkap dengan tag yang melekat pada buku."""

book_tags_small = book_tags_renamed[['book_id', 'tag_id']]

# Gabungkan data books_ratings dengan tag buku
books_ratings_with_tags = pd.merge(books_ratings, book_tags_small, on='book_id', how='left')
print(f"books_ratings_with_tags shape: {books_ratings_with_tags.shape}")

"""Memastikan jumlah buku unik yang ada di semua dataset sudah benar dan lengkap."""

to_read_small = to_read[['user_id', 'book_id']]

# Gabungkan dan cari jumlah buku unik dari semua dataset untuk validasi data
unique_books_all = np.unique(np.concatenate([
    books['book_id'].unique(),
    book_tags_renamed['book_id'].unique(),
    to_read['book_id'].unique(),
    ratings['book_id'].unique()
]))
print(f"Jumlah buku unik dari semua dataset: {len(unique_books_all)}")

"""Menghitung berapa banyak tag berbeda yang dimiliki setiap buku. Ini berguna untuk fitur analisis lebih lanjut."""

# Hitung jumlah tag unik per buku
tag_count_per_book = book_tags_renamed.groupby('book_id')['tag_id'].nunique().reset_index()
tag_count_per_book.rename(columns={'tag_id': 'tag_count'}, inplace=True)
print(tag_count_per_book.head())

"""Menggabungkan jumlah tag ke dataset rating + buku."""

# Gabungkan data rating buku dengan jumlah tag tiap buku
books_ratings_with_tagcount = pd.merge(books_ratings, tag_count_per_book, on='book_id', how='left')
print(books_ratings_with_tagcount.shape)
print(books_ratings_with_tagcount.head())

"""Menghitung berapa banyak user yang menandai buku sebagai "ingin dibaca" untuk setiap buku."""

# Hitung jumlah user yang ingin membaca tiap buku (to_read_count)
to_read_count = to_read.groupby('book_id')['user_id'].nunique().reset_index()
to_read_count.rename(columns={'user_id': 'to_read_count'}, inplace=True)
print(to_read_count.head())

"""Menggabungkan semua fitur dalam satu dataset lengkap dan mengisi nilai kosong supaya data siap dipakai.

"""

# Gabungkan semua data fitur (rating, tag_count, to_read_count) dalam satu dataframe
books_ratings_full = pd.merge(books_ratings_with_tagcount, to_read_count, on='book_id', how='left')

# Isi nilai kosong (NaN) pada to_read_count dengan 0 dan ubah tipe jadi integer
books_ratings_full['to_read_count'] = books_ratings_full['to_read_count'].fillna(0).astype(int)

print(books_ratings_full[['book_id', 'tag_count', 'to_read_count']].head())
print(f'Total rows after merge: {books_ratings_full.shape[0]}')

"""Pengecekan dan pembersihan nilai kosong agar tidak ada masalah saat analisis selanjutnya.

"""

# Cek nilai kosong (missing values) pada kolom penting
books_ratings_full.isnull().sum()

cols_needed = ['book_id', 'rating', 'user_id', 'tag_count', 'to_read_count']
books_subset = books_ratings_full[cols_needed]
print(books_subset.isnull().sum())

"""Membuat ringkasan statistik rating tiap buku sekaligus menyimpan fitur tambahan (tag dan to_read). Ini memudahkan analisis lebih lanjut."""

# Mengelompokkan data berdasarkan book_id untuk mendapatkan ringkasan rating dan fitur
books_grouped = books_ratings_full.groupby('book_id').agg({
    'rating': ['sum', 'mean', 'count'],
    'tag_count': 'first',
    'to_read_count': 'first'
}).reset_index()

# Rename columns
books_grouped.columns = ['book_id', 'rating_sum', 'rating_mean', 'rating_count', 'tag_count', 'to_read_count']

print(books_grouped.head())

"""Melakukan validasi data rating, memastikan hanya data dengan metadata buku lengkap yang dipakai agar tidak ada data rusak."""

# Mengecek book_id yang ada di ratings tapi tidak ada di books (metadata buku)
missing_books = books_ratings[~books_ratings['book_id'].isin(books['book_id'])]
print(missing_books['book_id'].value_counts())

missing_count = ratings[~ratings['book_id'].isin(books['book_id'])]['book_id'].nunique()
print(f"Jumlah book_id yang tidak ditemukan di metadata books: {missing_count}")

# Filter ratings hanya yang ada metadata bukunya
rating_filtered = ratings[ratings['book_id'].isin(books['book_id'])]
books_ratings_full = pd.merge(rating_filtered, books, on='book_id', how='left')

"""Membuat grafik distribusi rating untuk mengetahui sebaran rating dalam dataset."""

# Visualisasi distribusi rating buku
sns.histplot(ratings['rating'], bins=5, kde=True)
plt.title('Distribusi Rating Buku')
plt.xlabel('Rating')
plt.ylabel('Jumlah')
plt.show()

# Visualisasi distribusi jumlah rating per user
user_rating_counts = ratings['user_id'].value_counts()
sns.histplot(user_rating_counts, bins=100)
plt.title('Distribusi Rating per Pengguna')
plt.xlim(0, 100)  # Fokus pada pengguna dengan rating sampai 100 kali
plt.show()

# Mengatasi missing value pada dataset books
books.fillna({'language_code': books['language_code'].mode()[0]}, inplace=True)

"""# Data Preparation

Membaca dataset dari file CSV ke dalam DataFrame bernama *books.*
"""

books = pd.read_csv('./goodbooks10k/books.csv')

"""Mengecek nama kolom dan beberapa data teratas untuk memahami struktur data."""

print(books.columns)
print(books.head())

"""Menyaring kolom-kolom penting saja dari dataset, lalu membuat salinan ke book_new agar data asli tidak berubah."""

book_new = books[['book_id', 'title', 'authors', 'original_publication_year', 'average_rating', 'ratings_count']].copy()

"""Mengganti nama kolom menjadi lebih ringkas dan mudah digunakan."""

book_new.columns = ['id', 'title', 'author', 'year', 'avg_rating', 'num_ratings']

"""Menampilkan 10 data awal dan 5 data acak untuk melihat distribusi dan isi data."""

print(book_new.head(10))
print(book_new.sample(5))

"""Mengecek nilai kosong (missing values) di setiap kolom

"""

print(book_new.isnull().sum())

"""Menangani nilai kosong pada kolom year dengan mengisinya menggunakan nilai median. Ini adalah pendekatan umum untuk data numerik yang kemungkinan skewed."""

median_year = book_new['year'].median()
book_new['year'] = book_new['year'].fillna(median_year)

"""Mengecek ulang apakah masih ada nilai kosong setelah pengisian."""

print(book_new.isnull().sum())

"""# Model Development dengan Content Based Filtering

Menyiapkan dataset book_new dan menampilkan 5 sampel acak dari data untuk eksplorasi awal.
"""

data = book_new
data.sample(5)

"""* Membuat instance *TfidfVectorizer* untuk mengubah teks menjadi vektor numerik.

* *stop_words='english'* digunakan agar kata-kata umum dalam bahasa Inggris diabaikan (misalnya: "the", "and", dll).

* Melakukan **fit** dan **transform** pada kolom 'combined' yang merupakan gabungan dari judul dan nama penulis untuk menangkap konteks konten buku.

* Mengubah hasil TF-IDF menjadi DataFrame agar bisa divisualisasikan dan difilter.

* Memfilter hanya kolom dengan nilai TF-IDF lebih dari nol di setidaknya satu baris untuk mengurangi kolom yang tidak relevan.






"""

# Buat TF-IDF Vectorizer dengan stop words bahasa Inggris
tfidf = TfidfVectorizer(stop_words='english')

# Fit dan transform data 'combined' (judul + author)
tfidf_matrix = tfidf.fit_transform(data['combined'])

# Buat DataFrame dari tfidf_matrix untuk visualisasi (optional)
tfidf_df = pd.DataFrame.sparse.from_spmatrix(
    tfidf_matrix,
    index=data['title'],
    columns=tfidf.get_feature_names_out()
)

# Filter kolom yang ada nilai TF-IDF > 0 minimal di satu baris (kata yang relevan)
tfidf_nonzero = tfidf_df.loc[:, (tfidf_df > 0).any()]

print(tfidf_nonzero.head())

"""* Mengukur seberapa mirip antar buku berdasarkan vektor TF-IDF dengan metode cosine similarity.

* Mengubah hasil cosine similarity menjadi DataFrame agar bisa dicari berdasarkan judul.

"""

# Hitung cosine similarity matrix dari tfidf_matrix
cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)

# Ubah menjadi DataFrame dengan index dan kolom berupa judul buku
cosine_sim_df = pd.DataFrame(cosine_sim, index=data['title'], columns=data['title'])

print(cosine_sim_df.sample(5).sample(5, axis=1))  # lihat sample similarity

"""Fungsi ini menerima title dan mengembalikan daftar buku paling mirip berdasarkan konten (judul + penulis) menggunakan nilai cosine similarity tertinggi."""

def recommend_books(title, cosine_sim=cosine_sim_df, data=data, top_n=10):
    # Cek apakah buku ada di dataset
    if title not in cosine_sim.index:
        return f"Buku '{title}' tidak ditemukan dalam data."

    # Ambil skor similarity buku tersebut dengan buku lain
    sim_scores = cosine_sim[title]

    # Urutkan skor similarity secara menurun, kecuali dirinya sendiri (nilai max = 1)
    sim_scores = sim_scores.drop(title).sort_values(ascending=False)

    # Ambil top n buku yang paling mirip
    top_books = sim_scores.head(top_n).index

    # Ambil data buku tersebut
    recommended = data[data['title'].isin(top_books)][['title', 'author', 'year', 'avg_rating']]

    return recommended.reset_index(drop=True)

print(data['title'].head(20))

"""Menampilkan 10 buku paling mirip dengan The Hunger Games berdasarkan konten."""

print(recommend_books('The Hunger Games (The Hunger Games, #1)'))

"""#### Fungsi Generalisasi

* data: diasumsikan sebagai DataFrame utama yang berisi kolom title, author, avg_rating, dan year.

* cosine_sim_df: diasumsikan sebagai DataFrame berisi similarity scores antar buku (judul sebagai index & columns).
"""

def get_recommendations(method, value, top_n=5):
    if method == 'title':
        book_title = value
        if book_title not in data['title'].values:
            print(f"Buku '{book_title}' tidak ditemukan dalam data.")
            return

        if book_title not in cosine_sim_df.index:
            print(f"Buku '{book_title}' tidak ditemukan di cosine_sim_df index.")
            return

        sim_scores = cosine_sim_df.loc[book_title]
        closest_titles = sim_scores.nlargest(top_n + 1).index.drop(book_title)
        recommendations = data[data['title'].isin(closest_titles)].head(top_n).reset_index(drop=True)

        print("Rekomendasi Berdasarkan Judul Buku:")
        for i, row in recommendations.iterrows():
            print(f"{i+1}. {row['title']} - {row['author']}")
        print()  # Memberi jarak antar output

    elif method == 'author':
        author_books = data[data['author'].str.contains(value, case=False, na=False)]
        if author_books.empty:
            print(f"Tidak ditemukan buku oleh penulis '{value}'.")
            return

        top_books = author_books.sort_values(by='avg_rating', ascending=False).head(top_n).reset_index(drop=True)
        print(f"Rekomendasi Buku oleh Penulis '{value}':")
        for i, row in top_books.iterrows():
            print(f"{i+1}. {row['title']} - {row['author']} (Rating: {row['avg_rating']})")
        print()  # Memberi jarak antar output

    elif method == 'year':
        year_books = data[data['year'] == value]
        if year_books.empty:
            print(f"Tidak ada buku ditemukan dari tahun {value}.")
            return

        top_books = year_books.sort_values(by='avg_rating', ascending=False).head(top_n).reset_index(drop=True)
        print(f"Rekomendasi Buku dari Tahun {value}:")
        for i, row in top_books.iterrows():
            print(f"{i+1}. {row['title']} - {row['author']} (Rating: {row['avg_rating']})")
        print()  # Memberi jarak antar output

    else:
        print("Metode tidak dikenal. Gunakan 'title', 'author', atau 'year'.")

# Rekomendasi berdasarkan judul
get_recommendations(method='title', value='The Hunger Games (The Hunger Games, #1)', top_n=5)

# Rekomendasi berdasarkan penulis
get_recommendations(method='author', value='Suzanne Collins', top_n=5)

# Rekomendasi berdasarkan tahun
get_recommendations(method='year', value=2008, top_n=5)

"""# Training

**1. Preprocessing Data**

* Mengubah user ID dan book ID ke bentuk numerik dengan LabelEncoder.

* Membuat target biner: rating 4 ke atas dianggap suka (1), di bawahnya tidak suka (0).

* Membagi data menjadi data latih dan data validasi dengan proporsi 80:20 dan stratifikasi label agar seimbang.

**2. Membangun Model Neural Network**

* Model menggunakan embedding layer untuk merepresentasikan user dan buku sebagai vektor berdimensi rendah (embedding).

* Menghitung interaksi (dot product) antara embedding user dan buku untuk memperkirakan preferensi.

* Menambahkan bias khusus user dan buku untuk meningkatkan akurasi prediksi.

* Menggunakan fungsi aktivasi sigmoid untuk menghasilkan output probabilitas suka atau tidak.

**3. Kompilasi dan Pelatihan Model**

* Model dikompilasi dengan loss function binary crossentropy karena targetnya berupa kelas biner.

* Optimizer Adam dengan learning rate kecil digunakan untuk training.

* Monitoring metrik seperti akurasi, AUC, dan RMSE selama pelatihan.

* Callback seperti early stopping dan reduce learning rate on plateau dipakai untuk menghindari overfitting dan meningkatkan performa.

**4. Evaluasi Model**

* Menggunakan data validasi untuk mengevaluasi hasil prediksi model.

* Menghasilkan confusion matrix dan classification report untuk melihat performa klasifikasi.

* Memvisualisasikan ROC curve untuk mengukur kemampuan model dalam membedakan kelas.
"""

# === PREPROCESSING ===

# Label encoding untuk user dan book ID
user_enc = LabelEncoder()
book_enc = LabelEncoder()

ratings['user'] = user_enc.fit_transform(ratings['user_id'].values)
ratings['book'] = book_enc.fit_transform(ratings['book_id'].values)
ratings['rating'] = ratings['rating'].values.astype(np.float32)

# Membuat label biner: rating >= 4 dianggap suka (1), lainnya tidak suka (0)
x = ratings[['user', 'book']].values
y = ratings['rating'].apply(lambda r: 1 if r >= 4 else 0).values

# Split data menjadi training dan validation set dengan stratifikasi agar seimbang
x_train, x_val, y_train, y_val = train_test_split(
    x, y, test_size=0.2, random_state=42, stratify=y
)

# === MODEL DEFINITION ===

class RecommenderNet(tf.keras.Model):
    def __init__(self, num_users, num_books, embedding_size, **kwargs):
        super(RecommenderNet, self).__init__(**kwargs)

        # Embedding untuk user
        self.user_embedding = layers.Embedding(
            input_dim=num_users,
            output_dim=embedding_size,
            embeddings_initializer='he_normal',
            embeddings_regularizer=keras.regularizers.l2(1e-6)
        )
        self.user_bias = layers.Embedding(num_users, 1)

        # Embedding untuk book
        self.book_embedding = layers.Embedding(
            input_dim=num_books,
            output_dim=embedding_size,
            embeddings_initializer='he_normal',
            embeddings_regularizer=keras.regularizers.l2(1e-6)
        )
        self.book_bias = layers.Embedding(num_books, 1)

        # Dropout untuk regularisasi
        self.dropout = layers.Dropout(0.3)

    def call(self, inputs, training=False):
        user_vector = self.user_embedding(inputs[:, 0])
        user_bias = self.user_bias(inputs[:, 0])
        book_vector = self.book_embedding(inputs[:, 1])
        book_bias = self.book_bias(inputs[:, 1])

        # Perhitungan skor interaksi dengan dot product dan bias
        dot_user_book = tf.reduce_sum(user_vector * book_vector, axis=1, keepdims=True)
        x = dot_user_book + user_bias + book_bias

        x = self.dropout(x, training=training)
        return tf.nn.sigmoid(x)

# Jumlah user dan buku unik
num_users = ratings['user'].nunique()
num_books = ratings['book'].nunique()

# Inisialisasi model
model = RecommenderNet(num_users, num_books, embedding_size=64)

# Kompilasi model
model.compile(
    loss=tf.keras.losses.BinaryCrossentropy(),
    optimizer=keras.optimizers.Adam(learning_rate=0.00005),
    metrics=[
        tf.keras.metrics.BinaryAccuracy(name='accuracy'),
        tf.keras.metrics.AUC(name='auc'),
        tf.keras.metrics.RootMeanSquaredError(name='rmse')
    ]
)

# Callback untuk early stopping
early_stopping = tf.keras.callbacks.EarlyStopping(
    monitor='val_auc',
    patience=5,
    restore_best_weights=True
)

# Callback untuk mengurangi learning rate saat performa stagnan
reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(
    monitor='val_auc',
    factor=0.5,
    patience=3,
    min_lr=1e-6,
    verbose=1,
    mode='max'
)

# === TRAINING ===

history = model.fit(
    x=x_train,
    y=y_train,
    batch_size=256,
    epochs=40,
    validation_data=(x_val, y_val),
    callbacks=[early_stopping, reduce_lr]
)

# === EVALUATION ===

# Prediksi probabilitas
y_val_pred_prob = model.predict(x_val).flatten()

# Konversi ke label biner dengan threshold default 0.5
threshold = 0.5
y_val_pred = (y_val_pred_prob >= threshold).astype(int)

# Tampilkan confusion matrix dan classification report
print("Confusion Matrix:")
print(confusion_matrix(y_val, y_val_pred))

print("\nClassification Report:")
print(classification_report(y_val, y_val_pred))

# Plot ROC Curve
fpr, tpr, _ = roc_curve(y_val, y_val_pred_prob)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC)')
plt.legend(loc="lower right")
plt.show()

"""#### Menyimpan Model dan Encoder

* Model TensorFlow disimpan ke file recommender_model.h5 supaya bisa digunakan ulang tanpa training ulang.

* Encoder user dan book yang dipakai untuk mengubah ID asli menjadi indeks numerik juga disimpan menggunakan pickle.

* Penting untuk menyimpan encoder supaya saat rekomendasi bisa mengonversi ID user atau buku secara konsisten.
"""

# Simpan model
model.save("recommender_model.h5")

# Simpan encoder
import pickle

with open("user_encoder.pkl", "wb") as f:
    pickle.dump(user_enc, f)
with open("book_encoder.pkl", "wb") as f:
    pickle.dump(book_enc, f)

"""#### Fungsi Rekomendasi dengan Judul dan Penulis Buku

Fungsi ini merekomendasikan buku untuk user berdasarkan user_id input. Fungsi mengecek keberadaan user dalam data, lalu memprediksi skor kecocokan semua buku dengan user tersebut menggunakan model yang sudah dilatih. Fungsi mengembalikan dan menampilkan daftar buku terbaik (top N) berdasarkan skor prediksi, lengkap dengan judul dan penulisnya.
"""

def recommend_books_with_titles(user_id_input, top_n=5):
    """
    Rekomendasi buku berdasarkan user_id.
    Mengembalikan DataFrame buku rekomendasi top_n dengan judul dan penulis.
    """
    user_id = np.int64(user_id_input)

    # Cek apakah user ada dalam data pelatihan
    if user_id not in user_enc.classes_:
        print(f"User ID '{user_id}' tidak ditemukan dalam data pelatihan.")
        return None

    # Encode user_id
    encoded_user_id = user_enc.transform([user_id])[0]

    # Semua indeks buku dari 0 sampai num_books-1
    book_indices = np.arange(num_books)

    # Buat array user dengan nilai encoded_user_id sebanyak buku
    user_array = np.full_like(book_indices, fill_value=encoded_user_id)

    # Gabungkan user dan book indices jadi input untuk model
    inputs = np.vstack((user_array, book_indices)).T

    # Prediksi skor kecocokan buku untuk user
    predictions = model.predict(inputs).flatten()

    # Ambil indeks buku dengan skor tertinggi
    top_indices = predictions.argsort()[::-1][:top_n]

    # Mapping indeks buku ke ID asli buku di data
    book_id_index_to_id = data['id'].sort_values().values
    recommended_book_real_ids = book_id_index_to_id[top_indices]

    print("recommended_book_real_ids:", recommended_book_real_ids)

    # Cek ID buku yang tidak ditemukan dalam data (seharusnya kosong)
    missing_ids = [bid for bid in recommended_book_real_ids if bid not in data['id'].values]
    if missing_ids:
        print("Book IDs tidak ditemukan di data:", missing_ids)

    # Filter yang valid (ada dalam data)
    valid_book_ids = [bid for bid in recommended_book_real_ids if bid in data['id'].values]

    if len(valid_book_ids) < top_n:
        print(f"Warning: hanya {len(valid_book_ids)} buku ditemukan dari rekomendasi top-{top_n}")

    # Ambil data buku berdasarkan ID valid, urut sesuai urutan rekomendasi
    recommended_books = data[data['id'].isin(valid_book_ids)]
    recommended_books = recommended_books.set_index('id').loc[valid_book_ids].reset_index()

    # Tampilkan hasil rekomendasi
    print(f"Rekomendasi Top-{len(valid_book_ids)} untuk User {user_id}:")
    for i, row in recommended_books.iterrows():
        print(f"{i+1}. {row['title']} - {row['author']}")

    return recommended_books[['id', 'title', 'author']]

"""Fungsi menghasilkan daftar 5 buku yang direkomendasikan untuk user dengan ID 1, lengkap dengan judul dan nama penulisnya. Prediksi model menampilkan buku dengan skor kecocokan tertinggi sebagai rekomendasi teratas."""

# Contoh pemanggilan fungsi
recommended_books = recommend_books_with_titles(1)

