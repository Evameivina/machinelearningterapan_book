# -*- coding: utf-8 -*-
"""Sistem_Rekomendasi_Buku_Eva Meivina.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SuoogTZ0h9GS5CZlyAKAO5PBKEY0S-5I

# **Dataset goodbooks-10k dari Kaggle**

## IMPORT LIBRARY
"""

# Install Kaggle API untuk mengunduh dataset dari Kaggle
!pip install kaggle

# Import Libraries
import os
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

"""***Konfigurasi API Kaggle***"""

os.environ['KAGGLE_USERNAME'] = "evameivinadwiana"
os.environ['KAGGLE_KEY'] = "efbafcb258eb3a5f3033f21f925e06ef"

"""***Download dan Ekstrak Dataset***"""

# Download dataset goodbooks-10k dari Kaggle
!kaggle datasets download zygmunt/goodbooks-10k

# Extract file ZIP ke folder ./goodbooks10k (overwrite jika sudah ada)
!unzip -o goodbooks-10k.zip -d ./goodbooks10k

"""## DATA UNDERSTANDING

***Load dataset Goodbooks-10k***
"""

# Load dataset goodbooks-10k
books = pd.read_csv('./goodbooks10k/books.csv')
book_tags = pd.read_csv('./goodbooks10k/book_tags.csv')
ratings = pd.read_csv('./goodbooks10k/ratings.csv')
tags = pd.read_csv('./goodbooks10k/tags.csv')
to_read = pd.read_csv('./goodbooks10k/to_read.csv')

"""Lima dataset dimuat dari direktori Goodbooks-10k untuk proses eksplorasi lebih lanjut.

*Ringkasan Jumlah Data*
"""

# Tampilkan kolom dari book_tags untuk memastikan nama kolom yang benar
print(book_tags.columns)

# Informasi ringkas jumlah data di setiap dataset
print('Jumlah buku:', len(books))
print('Jumlah tag buku:', len(book_tags['goodreads_book_id'].unique())) # Assuming 'goodreads_book_id' is the correct column name based on common Kaggle datasets. Adjust if necessary.
print('Jumlah penilaian pengguna:', len(ratings))
print('Jumlah pengguna yang memberikan rating:', len(ratings.user_id.unique()))
print('Jumlah tag yang tersedia:', len(tags))
print('Jumlah daftar buku yang akan dibaca:', len(to_read))

"""***Contoh Data Buku dan Rating***"""

# Menampilkan beberapa baris awal dari dataset books dan ratings
print('\nContoh data buku:')
print(books.head())

print('\nContoh data rating:')
print(ratings.head())

"""***Informasi Detail dan Statistik Deskriptif***

* info() untuk melihat tipe data dan jumlah data yang tidak kosong (cek missing value).

* describe() untuk ringkasan statistik kolom numerik (mean, std, min, max, dll).

* head() untuk menampilkan beberapa baris pertama data sebagai contoh.
"""

print("===== BOOKS =====")
books.info()
print("\nDeskripsi Statistik:")
print(books.describe())
print("\nContoh Data:")
print(books.head())

print("\n===== BOOK_TAGS =====")
book_tags.info()
print("\nDeskripsi Statistik:")
print(book_tags.describe())
print("\nContoh Data:")
print(book_tags.head())

print("\n===== RATINGS =====")
ratings.info()
print("\nDeskripsi Statistik:")
print(ratings.describe())
print("\nContoh Data:")
print(ratings.head())

print("\n===== TAGS =====")
tags.info()
print("\nDeskripsi Statistik:")
print(tags.describe())
print("\nContoh Data:")
print(tags.head())

print("\n===== TO_READ =====")
to_read.info()
print("\nDeskripsi Statistik:")
print(to_read.describe())
print("\nContoh Data:")
print(to_read.head())

"""***Informasi Buku dan Penulis***"""

# Ringkasan tambahan untuk insight jumlah unik dan contoh data
print('Jumlah buku yang mendapatkan rating: ', len(ratings.book_id.unique()))
print('Jumlah penulis unik: ', books.authors.nunique())
print('Contoh penulis: ', books.authors.unique()[:10])

"""***Informasi Tag (Kategori Buku)***"""

print("Informasi variabel tags (jenis kategori buku):")
tags.info()

print("\nContoh data tag:")
print(tags.head())

print("\nJumlah tag unik yang tersedia:", tags['tag_name'].nunique())

print('Banyak tipe tag (kategori buku):', tags['tag_name'].nunique())
print('Tipe tag buku (kategori):')
print(tags['tag_name'].unique()[:50])

"""## EXPLORATORY DATA ANALYSIS (EDA)

***Ukuran Dataset***
"""

# Ukuran dataset ratings dan books
print(ratings.shape)
print(books.shape)

"""* Dataset ratings memiliki ukuran 981.756 baris dan 3 kolom
* Dataset books terdiri dari 10.000 baris dan 23 kolom

***Preview Data***
Menampilkan 5 baris pertama dari masing-masing dataset.
"""

# Preview beberapa baris pertama
ratings.head()
books.head()

"""***Deskripsi Statistik***"""

ratings.describe()
books.describe()

"""***Jumlah Unik Entitas***"""

print('Jumlah user_id (pengguna): ', len(ratings.user_id.unique()))
print('Jumlah book_id (buku): ', len(ratings.book_id.unique()))
print('Jumlah data rating: ', len(ratings))

"""## DATA PREPROCESSING

***Menampilkan Ukuran Dataset***
"""

# Menampilkan ukuran dataset utama
print("books shape:", books.shape)
print("to_read shape:", to_read.shape)
print("ratings shape:", ratings.shape)

"""***Menggabungkan dan Menghitung Seluruh book_id Unik***"""

# Menggabungkan seluruh book_id pada kategori Buku
books_all = np.concatenate((
    books.book_id.unique(),
    ratings.book_id.unique(),
    book_tags.goodreads_book_id.unique(),
    to_read.book_id.unique()
))

# Mengurutkan dan menghapus duplikat
books_all = np.sort(np.unique(books_all))
print('Jumlah seluruh buku unik berdasarkan book_id:', len(books_all))

"""***Menggabungkan dan Menghitung user_id Unik***"""

# Menggabungkan seluruh user_id pada kategori Pengguna
user_all = np.concatenate((
    ratings.user_id.unique(),
))

# Menghapus duplikat dan mengurutkan
user_all = np.sort(np.unique(user_all))
print('Jumlah seluruh pengguna unik berdasarkan user_id:', len(user_all))

"""***Menyamakan tipe data book_id agar konsisten untuk proses merge.***"""

# Ubah tipe data book_id menjadi integer agar konsisten dan siap digabungkan
books['book_id'] = books['book_id'].astype(int)
book_tags_renamed = book_tags.rename(columns={'goodreads_book_id': 'book_id'})
book_tags_renamed['book_id'] = book_tags_renamed['book_id'].astype(int)
to_read['book_id'] = to_read['book_id'].astype(int)
ratings['book_id'] = ratings['book_id'].astype(int)

"""***Menggabungkan Ratings dan Metadata Buku***"""

# Gabungkan data ratings dengan metadata buku berdasarkan book_id
books_ratings = pd.merge(ratings, books, on='book_id', how='left')
print(f"books_ratings shape: {books_ratings.shape}")

"""***Menambahkan Tag ID*** Menggabungkan tag dari book_tags ke data ratings+books"""

book_tags_small = book_tags_renamed[['book_id', 'tag_id']]

# Gabungkan data books_ratings dengan tag buku
books_ratings_with_tags = pd.merge(books_ratings, book_tags_small, on='book_id', how='left')
print(f"books_ratings_with_tags shape: {books_ratings_with_tags.shape}")

to_read_small = to_read[['user_id', 'book_id']]

# Gabungkan dan cari jumlah buku unik dari semua dataset untuk validasi data
unique_books_all = np.unique(np.concatenate([
    books['book_id'].unique(),
    book_tags_renamed['book_id'].unique(),
    to_read['book_id'].unique(),
    ratings['book_id'].unique()
]))
print(f"Jumlah buku unik dari semua dataset: {len(unique_books_all)}")

"""***Menghitung Jumlah Tag Unik per Buku***"""

# Hitung jumlah tag unik per buku
tag_count_per_book = book_tags_renamed.groupby('book_id')['tag_id'].nunique().reset_index()
tag_count_per_book.rename(columns={'tag_id': 'tag_count'}, inplace=True)
print(tag_count_per_book.head())

"""***Menambahkan Informasi Jumlah Tag ke Dataset Ratings***"""

# Gabungkan data rating buku dengan jumlah tag tiap buku
books_ratings_with_tagcount = pd.merge(books_ratings, tag_count_per_book, on='book_id', how='left')
print(books_ratings_with_tagcount.shape)
print(books_ratings_with_tagcount.head())

"""***Menghitung Jumlah User yang Ingin Membaca Buku***"""

# Hitung jumlah user yang ingin membaca tiap buku (to_read_count)
to_read_count = to_read.groupby('book_id')['user_id'].nunique().reset_index()
to_read_count.rename(columns={'user_id': 'to_read_count'}, inplace=True)
print(to_read_count.head())

"""***Finalisasi Dataset dengan Menyatukan Semua Komponen & Menangani Missing Values***"""

# Gabungkan data rating dan tag_count dengan to_read_count berdasarkan 'book_id'
books_ratings_full = pd.merge(books_ratings_with_tagcount, to_read_count, on='book_id', how='left')

# Isi missing values pada 'to_read_count' dengan 0 dan ubah tipe data jadi integer
books_ratings_full['to_read_count'] = books_ratings_full['to_read_count'].fillna(0).astype(int)

# Cek missing values pada kolom penting
cols_needed = ['book_id', 'user_id', 'rating', 'tag_count', 'to_read_count']
books_subset = books_ratings_full[cols_needed]
print("Missing values sebelum imputasi 'tag_count':")
print(books_subset.isnull().sum())

# Isi missing values di kolom 'tag_count' dengan median untuk menghindari bias yang terlalu ekstrem
median_tag_count = books_ratings_full['tag_count'].median()
books_ratings_full['tag_count'] = books_ratings_full['tag_count'].fillna(median_tag_count)

# Ambil subset kolom penting setelah imputasi
books_clean = books_ratings_full[cols_needed].copy()

# Pastikan tidak ada missing values lagi
print("\nMissing values setelah imputasi dan pembersihan:")
print(books_clean.isnull().sum())

# Drop baris yang masih mengandung missing values (jika ada)
books_clean.dropna(inplace=True)

# Tampilkan contoh data yang sudah bersih dan siap pakai
print("\nContoh data bersih siap untuk modeling:")
print(books_clean.head())

"""Melakukan validasi data rating, memastikan hanya data dengan metadata buku lengkap yang dipakai agar tidak ada data rusak."""

# Mengecek book_id yang ada di ratings tapi tidak ada di books (metadata buku)
missing_books = books_ratings[~books_ratings['book_id'].isin(books['book_id'])]
print(missing_books['book_id'].value_counts())

missing_count = ratings[~ratings['book_id'].isin(books['book_id'])]['book_id'].nunique()
print(f"Jumlah book_id yang tidak ditemukan di metadata books: {missing_count}")

# Filter ratings hanya yang ada metadata bukunya
rating_filtered = ratings[ratings['book_id'].isin(books['book_id'])]
books_ratings_full = pd.merge(rating_filtered, books, on='book_id', how='left')

"""***Membuat grafik distribusi rating untuk mengetahui sebaran rating dalam dataset.***"""

# Visualisasi distribusi rating buku
sns.histplot(ratings['rating'], bins=5, kde=True)
plt.title('Distribusi Rating Buku')
plt.xlabel('Rating')
plt.ylabel('Jumlah')
plt.show()

# Visualisasi distribusi jumlah rating per user
user_rating_counts = ratings['user_id'].value_counts()
sns.histplot(user_rating_counts, bins=100)
plt.title('Distribusi Rating per Pengguna')
plt.xlim(0, 100)  # Fokus pada pengguna dengan rating sampai 100 kali
plt.show()

# Mengatasi missing value pada dataset books
books.fillna({'language_code': books['language_code'].mode()[0]}, inplace=True)

"""## DATA PREPARATION

Membaca dataset dari file CSV ke dalam DataFrame bernama *books.*
"""

books = pd.read_csv('./goodbooks10k/books.csv')

"""Mengecek nama kolom dan beberapa data teratas untuk memahami struktur data."""

print(books.columns)
print(books.head())

"""Menyaring kolom-kolom penting saja dari dataset, lalu membuat salinan ke book_new agar data asli tidak berubah."""

book_new = books[['book_id', 'title', 'authors', 'original_publication_year', 'average_rating', 'ratings_count']].copy()

"""Mengganti nama kolom menjadi lebih ringkas dan mudah digunakan."""

book_new.columns = ['id', 'title', 'author', 'year', 'avg_rating', 'num_ratings']

"""Menampilkan 10 data awal dan 5 data acak untuk melihat distribusi dan isi data."""

print(book_new.head(10))
print(book_new.sample(5))

"""Mengecek nilai kosong (missing values) di setiap kolom

"""

print(book_new.isnull().sum())

"""Menangani nilai kosong pada kolom year dengan mengisinya menggunakan nilai median. Ini adalah pendekatan umum untuk data numerik yang kemungkinan skewed."""

median_year = book_new['year'].median()
book_new['year'] = book_new['year'].fillna(median_year)

"""Mengecek ulang apakah masih ada nilai kosong setelah pengisian."""

print(book_new.isnull().sum())

"""## MODELLING & RESULTS

***Content Based Filtering***

Menyiapkan dataset buku agar hanya berisi informasi penting yang diperlukan untuk membangun sistem rekomendasi berbasis konten (content-based filtering).
"""

# === Data Preparation for Content-Based Filtering ===
# Menyaring kolom-kolom penting saja dari dataset, lalu membuat salinan ke book_new agar data asli tidak berubah.
book_new = books[['book_id', 'title', 'authors', 'original_publication_year', 'average_rating', 'ratings_count']].copy()

# Mengganti nama kolom menjadi lebih ringkas dan mudah digunakan.
book_new.columns = ['id', 'title', 'author', 'year', 'avg_rating', 'num_ratings']

"""Mengatasi missing value pada kolom year agar proses analisis dan pemodelan tidak terganggu, serta menyiapkan dataframe akhir untuk digunakan pada tahap berikutnya."""

# Menangani nilai kosong pada kolom year dengan mengisinya menggunakan nilai median.
median_year = book_new['year'].median()
book_new['year'] = book_new['year'].fillna(median_year)

# Assign the prepared dataframe to the variable 'data' which is used later
data = book_new

# Print head to verify the data
print("Prepared 'data' dataframe head:")
print(data.head())

"""Membangun model content-based filtering dengan memanfaatkan informasi teks dari judul dan penulis buku menggunakan metode TF-IDF (Term Frequency-Inverse Document Frequency). Model ini akan mengukur kemiripan antar buku berdasarkan konten teks untuk keperluan rekomendasi.


"""

# === Content Based Filtering ===
# Buat TF-IDF Vectorizer dengan stop words bahasa Inggris
tfidf = TfidfVectorizer(stop_words='english')

# Gabungkan kolom 'title' dan 'author' menjadi kolom baru 'combined'
# Ensure 'combined' column exists
if 'combined' not in data.columns:
    data['combined'] = data['title'].fillna('') + ' ' + data['author'].fillna('') # Handle potential NaNs

# Fit dan transform data 'combined' (judul + author)
tfidf_matrix = tfidf.fit_transform(data['combined'])

# Buat DataFrame dari tfidf_matrix untuk visualisasi (optional)
tfidf_df = pd.DataFrame.sparse.from_spmatrix(
    tfidf_matrix,
    index=data['title'],
    columns=tfidf.get_feature_names_out()
)

# Filter kolom yang ada nilai TF-IDF > 0 minimal di satu baris (kata yang relevan)
tfidf_nonzero = tfidf_df.loc[:, (tfidf_df > 0).any()]

print("\nTF-IDF Non-Zero Matrix Head:")
print(tfidf_nonzero.head())

"""Menghitung kemiripan antar buku berdasarkan representasi TF-IDF dari teks gabungan (judul + penulis) menggunakan cosine similarity dan membuat fungsi rekomendasi yang mengembalikan daftar buku paling mirip."""

# Hitung cosine similarity matrix dari tfidf_matrix
cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)

# Ubah menjadi DataFrame dengan index dan kolom berupa judul buku
cosine_sim_df = pd.DataFrame(cosine_sim, index=data['title'], columns=data['title'])

print(cosine_sim_df.sample(5).sample(5, axis=1))  # lihat sample similarity

# Fungsi ini menerima title dan mengembalikan daftar buku paling mirip berdasarkan konten (judul + penulis) menggunakan nilai cosine similarity tertinggi.
def recommend_books(title, cosine_sim=cosine_sim_df, data=data, top_n=10):
    # Cek apakah buku ada di dataset dan di cosine_sim index
    if title not in data['title'].values:
        return f"Buku '{title}' tidak ditemukan dalam data."

    if title not in cosine_sim.index:
         return f"Buku '{title}' tidak ditemukan dalam cosine similarity index."

    # Ambil skor similarity buku tersebut dengan buku lain
    sim_scores = cosine_sim[title]

    # Urutkan skor similarity secara menurun, kecuali dirinya sendiri (nilai max = 1)
    # Use .drop(title, errors='ignore') to handle cases where title might not be in the index (though unlikely here)
    sim_scores = sim_scores.drop(title, errors='ignore').sort_values(ascending=False)


    # Ambil top n buku yang paling mirip
    top_books_titles = sim_scores.head(top_n).index

    # Ambil data buku tersebut dari the main data DataFrame
    # Ensure the comparison is correct, comparing titles
    recommended = data[data['title'].isin(top_books_titles)][['title', 'author', 'year', 'avg_rating']].copy()

    return recommended.reset_index(drop=True)

print(data['title'].head(20))

"""Merekomendasikan buku-buku yang paling mirip berdasarkan konten (judul dan penulis) dari sebuah buku input menggunakan matriks cosine similarity."""

def recommend_books(title, cosine_sim=cosine_sim_df, data=data, top_n=10):
    # Cek apakah buku ada di dataset
    if title not in cosine_sim.index:
        return f"Buku '{title}' tidak ditemukan dalam data."

    # Ambil skor similarity buku tersebut dengan buku lain
    sim_scores = cosine_sim[title]

    # Urutkan skor similarity secara menurun, kecuali dirinya sendiri (nilai max = 1)
    sim_scores = sim_scores.drop(title).sort_values(ascending=False)

    # Ambil top n buku yang paling mirip
    top_books = sim_scores.head(top_n).index

    # Ambil data buku tersebut
    recommended = data[data['title'].isin(top_books)][['title', 'author', 'year', 'avg_rating']]

    return recommended.reset_index(drop=True)

print(data['title'].head(20))

"""*Menampilkan 10 buku paling mirip dengan The Hunger Games berdasarkan konten.*"""

print(recommend_books('The Hunger Games (The Hunger Games, #1)'))

"""emberikan rekomendasi buku yang relevan berdasarkan tiga metode pencarian: judul buku, penulis, dan tahun terbit."""

def get_recommendations(method, value, top_n=5):
    if method == 'title':
        book_title = value
        if book_title not in data['title'].values:
            print(f"Buku '{book_title}' tidak ditemukan dalam data.")
            return

        if book_title not in cosine_sim_df.index:
            print(f"Buku '{book_title}' tidak ditemukan di cosine_sim_df index.")
            return

        sim_scores = cosine_sim_df.loc[book_title]
        closest_titles = sim_scores.nlargest(top_n + 1).index.drop(book_title)
        recommendations = data[data['title'].isin(closest_titles)].head(top_n).reset_index(drop=True)

        print("Rekomendasi Berdasarkan Judul Buku:")
        for i, row in recommendations.iterrows():
            print(f"{i+1}. {row['title']} - {row['author']}")
        print()

    elif method == 'author':
        author_books = data[data['author'].str.contains(value, case=False, na=False)]
        if author_books.empty:
            print(f"Tidak ditemukan buku oleh penulis '{value}'.")
            return

        top_books = author_books.sort_values(by='avg_rating', ascending=False).head(top_n).reset_index(drop=True)
        print(f"Rekomendasi Buku oleh Penulis '{value}':")
        for i, row in top_books.iterrows():
            print(f"{i+1}. {row['title']} - {row['author']} (Rating: {row['avg_rating']})")
        print()

    elif method == 'year':
        year_books = data[data['year'] == value]
        if year_books.empty:
            print(f"Tidak ada buku ditemukan dari tahun {value}.")
            return

        top_books = year_books.sort_values(by='avg_rating', ascending=False).head(top_n).reset_index(drop=True)
        print(f"Rekomendasi Buku dari Tahun {value}:")
        for i, row in top_books.iterrows():
            print(f"{i+1}. {row['title']} - {row['author']} (Rating: {row['avg_rating']})")
        print()  # Memberi jarak antar output

    else:
        print("Metode tidak dikenal. Gunakan 'title', 'author', atau 'year'.")

# Rekomendasi berdasarkan judul
get_recommendations(method='title', value='The Hunger Games (The Hunger Games, #1)', top_n=5)

# Rekomendasi berdasarkan penulis
get_recommendations(method='author', value='Suzanne Collins', top_n=5)

# Rekomendasi berdasarkan tahun
get_recommendations(method='year', value=2008, top_n=5)

"""***Uji dengan judul yang kurang populer***"""

get_recommendations(method='title', value='The Quillan Games (Pendragon, #7)', top_n=5)

"""***Uji dengan penulis yang banyak karya***"""

get_recommendations(method='author', value='Rick Riordan', top_n=5)

get_recommendations(method='year', value=1900, top_n=5)

get_recommendations(method='year', value=2025, top_n=5)

"""***Menampilkan contoh output rekomendasi buku (content-based filtering)***"""

print("\nCONTOH OUTPUT TOP-N REKOMENDASI CONTENT-BASED FILTERING")
print("=" * 70)

if 'title' in data.columns and not data.empty:
    if len(data) > 5:
        test_title = data['title'].iloc[5]
        print(f"Testing Content-Based dengan buku: '{test_title}'")

        content_recommendations = recommend_books(test_title, top_n=5)

        if content_recommendations is not None and not content_recommendations.empty:
            print(f"\n CONTENT-BASED RECOMMENDATIONS FOR: '{test_title}'")
            print("=" * 70)
            for i, row in content_recommendations.iterrows():
                print(f"{i}. {row['title']}")
                print(f"   Author: {row.get('author', 'Unknown')}")
                print(f"   Rating: {row.get('avg_rating', 'N/A')}")
                print("-" * 50)
        else:
            print(f"No content-based recommendations available for '{test_title}'.")
    else:
        print("Data does not have enough rows to select index 5.")
else:
    print("Data DataFrame is empty or missing the 'title' column.")

"""## EVALUATION

***Evaluasi precision pada content-based filtering***

Mengukur performa sistem rekomendasi berbasis konten (content-based filtering) menggunakan metrik evaluasi Precision, Recall, dan F1-Score pada top-k rekomendasi.
"""

def evaluate_cbf_metrics_at_k(cosine_sim_df, data, k=5, sample_size=30):
    print(f"Evaluating Content-Based Filtering dengan Precision, Recall, dan F1-Score @ {k}")

    if data.empty:
        print("Data kosong, tidak bisa evaluasi.")
        return None, None, None

    if 'author' not in data.columns:
        print("Kolom 'author' tidak ditemukan di data, evaluasi tidak bisa dilakukan.")
        return None, None, None

    sample_books = data.sample(n=min(sample_size, len(data)), random_state=42)
    precisions, recalls, f1_scores = [], [], []

    for _, book_row in sample_books.iterrows():
        book_title = book_row['title']
        book_author = book_row['author']

        # Ambil rekomendasi top-k
        recs = recommend_books(book_title, cosine_sim_df, data, top_n=k) # Corrected

        if recs is not None and not recs.empty:
            relevant_count = sum(recs['author'] == book_author)
            precision = relevant_count / k

            # Recall: berapa proporsi buku relevan yang berhasil direkomendasikan
            total_relevant = sum(data['author'] == book_author) - 1
            recall = relevant_count / total_relevant if total_relevant > 0 else 0

            # F1-score: harmonic mean precision & recall
            if precision + recall > 0:
                f1 = 2 * (precision * recall) / (precision + recall)
            else:
                f1 = 0

            precisions.append(precision)
            recalls.append(recall)
            f1_scores.append(f1)

    if precisions:
        avg_precision = np.mean(precisions)
        avg_recall = np.mean(recalls)
        avg_f1 = np.mean(f1_scores)

        print(f"Average Precision@{k}: {avg_precision:.4f}")
        print(f"Average Recall@{k}: {avg_recall:.4f}")
        print(f"Average F1-Score@{k}: {avg_f1:.4f}")
        print(f"Evaluasi dilakukan pada {len(precisions)} buku sampel\n")
        return avg_precision, avg_recall, avg_f1
    else:
        print("Tidak ada data untuk evaluasi.")
        return None, None, None


# Contoh panggil dan bandingkan hasil untuk k=3,5,10,20
for k_value in [3, 5, 10, 20]:

    p, r, f1 = evaluate_cbf_metrics_at_k(cosine_sim_df, data, k=k_value, sample_size=30)
    print(f"Hasil evaluasi Precision@{k_value}: {p}")

"""Mengambil sampel buku dari dataset, kemudian untuk setiap buku menghitung seberapa banyak rekomendasi yang diberikan sesuai dengan penulis asli buku tersebut (sebagai indikator relevansi). Precision mengukur proporsi rekomendasi yang relevan dari keseluruhan rekomendasi top-k, Recall mengukur proporsi rekomendasi relevan dari seluruh buku relevan yang ada, dan F1-Score merupakan rata-rata harmonik dari Precision dan Recall. Evaluasi ini membantu menilai kualitas dan akurasi rekomendasi pada berbagai nilai k (jumlah rekomendasi yang diberikan)."""

