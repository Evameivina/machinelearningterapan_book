# -*- coding: utf-8 -*-
"""Revisi2_Sistem_Rekomendasi_Buku.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DoStNaSbDU4bN_Vjiz9qhYTsZzzK5e78

# **Dataset goodbooks-10k dari Kaggle**

## Import Library
"""

# Install Kaggle API untuk mengunduh dataset dari Kaggle
!pip install kaggle

"""*Mengimpor library yang dibutuhkan untuk analisis data dan pembuatan model rekomendasi*"""

import os
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import MinMaxScaler
from scipy.sparse import csr_matrix, hstack

"""*Set Environment Variables untuk API Kaggle*"""

os.environ['KAGGLE_USERNAME'] = "evameivinadwiana"
os.environ['KAGGLE_KEY'] = "efbafcb258eb3a5f3033f21f925e06ef"

"""Menetapkan Kaggle API berupa username dan API key sebagai environment variables agar dapat mengakses dan mengunduh dataset dari Kaggle secara otomatis.

*Download Dataset dan Extract Dataset ZIP dari Kaggle*
"""

# Download dataset goodbooks-10k dari Kaggle
!kaggle datasets download zygmunt/goodbooks-10k

# Extract file ZIP ke folder ./goodbooks10k (overwrite jika sudah ada)
!unzip -o goodbooks-10k.zip -d ./goodbooks10k

"""# **Data Understanding**

### **Memuat Dataset**

Memuat beberapa file CSV dari dataset Goodbooks-10k yang sudah diunduh dan diekstrak
"""

# Load dataset goodbooks-10k
books = pd.read_csv('./goodbooks10k/books.csv')
book_tags = pd.read_csv('./goodbooks10k/book_tags.csv')
ratings = pd.read_csv('./goodbooks10k/ratings.csv')
tags = pd.read_csv('./goodbooks10k/tags.csv')
to_read = pd.read_csv('./goodbooks10k/to_read.csv')

"""### **Memastikan Struktur Data**

Memeriksa nama kolom di *book_tags* untuk memastikan struktur data sudah benar:
"""

# Tampilkan kolom dari book_tags untuk memastikan nama kolom yang benar
print(book_tags.columns)

"""### **Statistik Ringkas Dataset**

Menampilkan jumlah data untuk setiap dataset utama
"""

# Informasi ringkas jumlah data di setiap dataset
print('Jumlah buku:', len(books))
print('Jumlah tag buku:', len(book_tags['goodreads_book_id'].unique())) # Assuming 'goodreads_book_id' is the correct column name based on common Kaggle datasets. Adjust if necessary.
print('Jumlah penilaian pengguna:', len(ratings))
print('Jumlah pengguna yang memberikan rating:', len(ratings.user_id.unique()))
print('Jumlah tag yang tersedia:', len(tags))
print('Jumlah daftar buku yang akan dibaca:', len(to_read))

"""Melihat 5 baris pertama dari setiap dataset untuk mendapatkan gambaran isi data"""

print(books.head())
print(book_tags.head())
print(ratings.head())
print(tags.head())
print(to_read.head())

"""Memeriksa apakah ada data yang kosong di setiap dataset untuk memastikan data bersih dan siap diproses"""

print(books.isnull().sum())
print(book_tags.isnull().sum())
print(ratings.isnull().sum())
print(tags.isnull().sum())
print(to_read.isnull().sum())

"""## **Univariate Exploratory Data Analysis**

**Dataset Overview**
"""

print("Dataset Dimensions:")
print(f"Ratings: {ratings.shape}")
print(f"Books: {books.shape}")

"""**Eksplorasi Dataset**"""

print("\n[Dataset: BOOKS]")
print("-" * 50)

# Menampilkan struktur data dan tipe tiap kolom
print("Struktur dan Tipe Data:")
books_structure = books.info()

# Menampilkan 5 data teratas sebagai sampel
print("\nContoh Data (5 baris pertama):")
print(books.head())

# Menampilkan ringkasan statistik numerik
print("\nStatistik Deskriptif:")
print(books.describe())

# Menampilkan informasi nilai yang hilang (jika ada)
print("\nNilai yang Hilang:")
missing_data = books.isnull().sum()
missing_data = missing_data[missing_data > 0]
if not missing_data.empty:
    print(missing_data)
else:
    print("Tidak ditemukan nilai kosong pada dataset.")

print("\n[Analisis Dataset: BOOK_TAGS]")
print("-" * 50)

# Menampilkan informasi struktur dataset
print("Detail Struktur Dataset:")
book_tags_structure = book_tags.info()

# Menampilkan beberapa data awal sebagai representasi
print("\nCuplikan Awal Data:")
print(book_tags.head())

# Menampilkan ringkasan statistik numerik
print("\nRingkasan Statistik:")
print(book_tags.describe())

# Menampilkan informasi nilai kosong (jika ada)
print("\nCek Data Kosong:")
missing_entries = book_tags.isnull().sum()
null_summary = missing_entries[missing_entries > 0]
if not null_summary.empty:
    print(null_summary)
else:
    print("Seluruh kolom telah terisi dengan lengkap.")

print("\n[Eksplorasi Awal: DATASET RATINGS]")
print("-" * 50)

# Menampilkan struktur data dan tipe setiap kolom
print("Struktur Kolom dan Tipe Data:")
ratings.info()

# Menampilkan lima baris pertama dari dataset
print("\nContoh Data:")
print(ratings.head())

# Menyajikan statistik ringkasan untuk kolom numerik
print("\nStatistik Umum:")
print(ratings.describe())

# Mengecek apakah ada data yang hilang
print("\nPemeriksaan Nilai Kosong:")
missing_data = ratings.isnull().sum()
missing_filtered = missing_data[missing_data > 0]
if not missing_filtered.empty:
    print(missing_filtered)
else:
    print("Tidak ditemukan nilai kosong dalam dataset.")

print("\n[Analisis Dataset: TAGS]")
print("-" * 50)

# Menampilkan struktur kolom dan tipe datanya
print("Detail Struktur Data:")
tags.info()

# Menampilkan data contoh awal
print("\nCuplikan Data:")
print(tags.head())

# Menampilkan ringkasan statistik numerik
print("\nRingkasan Statistik:")
print(tags.describe())

# Menampilkan kolom yang memiliki nilai kosong
print("\nCek Data Kosong:")
missing_info = tags.isnull().sum()
missing_found = missing_info[missing_info > 0]
if not missing_found.empty:
    print(missing_found)
else:
    print("Semua kolom telah terisi dengan baik.")

print("\n[Review Dataset: TO_READ]")
print("-" * 40)

# Menampilkan info struktur dataset
print("Detail Informasi Dataset:")
to_read.info()

# Menampilkan beberapa baris data pertama
print("\nData Sampel:")
print(to_read.head())

# Menampilkan statistik deskriptif untuk kolom numerik
print("\nStatistik Ringkas:")
print(to_read.describe())

# Mengecek dan menampilkan kolom yang mengandung nilai kosong
print("\nPemeriksaan Data Kosong:")
missing_entries = to_read.isnull().sum()
missing_cols = missing_entries[missing_entries > 0]
if not missing_cols.empty:
    print(missing_cols)
else:
    print("Tidak ditemukan nilai kosong pada dataset ini.")

"""*Memvisualisasikan 10 penulis dengan jumlah buku terbanyak pada dataset books.*"""

plt.style.use('default')
sns.set_palette("husl")
fig_size = (12, 6)
plt.figure(figsize=fig_size)

top_authors = books['authors'].value_counts().head(10)

plt.subplot(1, 2, 1)
top_authors.plot(kind='bar', color='skyblue')
plt.title('Top 10 Penulis dengan Buku Terbanyak')
plt.xlabel('Penulis')
plt.ylabel('Jumlah Buku')
plt.xticks(rotation=45)

"""Dari diagram batang yang dihasilkan, terlihat bahwa Stephen King menempati posisi teratas sebagai penulis dengan jumlah buku terbanyak dalam dataset ini. Posisi kedua diduduki oleh Nora Roberts, diikuti oleh Dean Koontz yang berada di peringkat ketiga. Ketiga penulis ini menjadi yang paling dominan, mencerminkan kontribusi mereka yang signifikan terhadap koleksi buku yang tercatat dalam dataset.

*Menampilkan 10 buku dengan rating rata-rata tertinggi dari dataset*
"""

top_rated = books.sort_values(by='average_rating', ascending=False).head(10)

plt.figure(figsize=(12, 6))
sns.barplot(data=top_rated, x='average_rating', y='title', palette='husl')
plt.title('10 Buku dengan Rating Tertinggi')
plt.xlabel('Rating')
plt.ylabel('Judul Buku')
plt.tight_layout()
plt.show()

"""Buku-buku tersebut diurutkan berdasarkan nilai *average_rating* secara menurun, sehingga buku dengan rating tertinggi muncul di bagian paling atas grafik. Dari visualisasi ini, buku dengan rating tertinggi adalah The Complete Calvin and Hobbes, yang menduduki posisi pertama.

*Menggabungkan dua dataset, yaitu book_tags dan tags, berdasarkan kolom tag_id menggunakan fungsi pd.merge()*
"""

merged_tags = pd.merge(book_tags, tags, on='tag_id')
top_tags = merged_tags.groupby('tag_name')['count'].sum().sort_values(ascending=False).head(10)

plt.figure(figsize=(12, 6))
top_tags.plot(kind='bar', color='mediumpurple')
plt.title('10 Tag Buku Paling Populer')
plt.xlabel('Tag')
plt.ylabel('Jumlah Penggunaan')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""Dari diagram batang tersebut, terlihat 10 tag buku paling populer berdasarkan frekuensi penggunaannya dalam dataset. Tag dengan jumlah penggunaan tertinggi adalah "to-read", yang menunjukkan bahwa banyak buku diberi label ini sebagai daftar bacaan yang ingin dibaca oleh pengguna.

*Distribusi Rating Pengguna*
"""

plt.figure(figsize=(8, 4))
sns.countplot(data=ratings, x='rating', palette='husl')
plt.title('Distribusi Rating yang Diberikan Pengguna')
plt.xlabel('Rating')
plt.ylabel('Jumlah')
plt.tight_layout()
plt.show()

"""Dari diagram distribusi rating ini, dapat dilihat bahwa rating dengan frekuensi tertinggi adalah rating 4. Artinya, mayoritas pengguna memberikan penilaian 4 pada buku-buku dalam dataset, menunjukkan bahwa secara umum buku-buku tersebut mendapat respon positif dengan rating yang cenderung tinggi.

*Menampilkan 10 buku yang paling sering dimasukkan ke dalam daftar "To-Read" oleh pengguna*
"""

to_read_count = to_read['book_id'].value_counts().head(10)
top_to_read_books = books[books['book_id'].isin(to_read_count.index)][['book_id', 'title']]
top_to_read_books['to_read_count'] = top_to_read_books['book_id'].map(to_read_count)

plt.figure(figsize=(12, 6))
sns.barplot(data=top_to_read_books, x='to_read_count', y='title', palette='Blues_d')
plt.title('10 Buku Paling Banyak Masuk Daftar "To-Read"')
plt.xlabel('Jumlah To-Read')
plt.ylabel('Judul Buku')
plt.tight_layout()
plt.show()

"""Diagram batang ini menampilkan 10 buku yang paling sering dimasukkan ke dalam daftar *To-Read* oleh pengguna. Buku-buku tersebut diurutkan berdasarkan frekuensi kemunculannya dalam daftar tersebut, dengan buku *The Hitchhiker's Guide to the Galaxy* menempati posisi teratas sebagai buku yang paling banyak dimasukkan.

*Analisis Kualitas Data pada Dataset Buku*
"""

datasets = {
    "Books": books,
    "Book Tags": book_tags,
    "Ratings": ratings,
    "Tags": tags,
    "To Read": to_read
}

def assess_data_quality(df, name):
    print(f"\n{name}:")

    total_cells = df.shape[0] * df.shape[1]
    missing_cells = df.isnull().sum().sum()
    missing_percent = (missing_cells / total_cells) * 100

    duplicate_rows = df.duplicated().sum()

    print(f"- Total Records     : {df.shape[0]}")
    print(f"- Total Columns     : {df.shape[1]}")
    print(f"- Missing Values    : {missing_cells}/{total_cells} ({missing_percent:.2f}%)")
    print(f"- Duplicate Rows    : {duplicate_rows}")

    numeric_cols = df.select_dtypes(include=[np.number]).columns
    if len(numeric_cols) > 0:
        print(f"- Numeric Columns   : {len(numeric_cols)}")
        for col in numeric_cols:
            Q1 = df[col].quantile(0.25)
            Q3 = df[col].quantile(0.75)
            IQR = Q3 - Q1
            outliers = df[(df[col] < Q1 - 1.5*IQR) | (df[col] > Q3 + 1.5*IQR)][col].count()
            if outliers > 0:
                print(f"  - {col}: {outliers} potential outliers")

for name, df in datasets.items():
    assess_data_quality(df, name)

"""# **Data Preparation**

Dilakukan pembersihan data dengan mengisi nilai kosong pada  *original_publication_year* menggunakan rata-rata dan *language_code* menggunakan modus. Kolom *isbn* dan *isbn13* yang banyak missing dihapus, serta data duplikat dihapus.

*Fitur authors* dan *language_code* diubah menjadi angka dengan *LabelEncoder*, dan fitur numerik dinormalisasi dengan *MinMaxScaler*.

Fitur numerik digabungkan dengan representasi *TF-IDF* dari judul buku untuk membentuk matriks fitur gabungan.

Matriks ini digunakan untuk menghitung *similarity matrix* berbasis cosine similarity guna analisis dan rekomendasi buku.
"""

print(f"- books null:\n{books.isnull().sum()}")
print(f"- ratings null:\n{ratings.isnull().sum()}")

books['original_publication_year'] = books['original_publication_year'].fillna(books['original_publication_year'].mean())

books_clean = books.drop(columns=['isbn', 'isbn13'])

books_clean = books_clean.drop_duplicates()
ratings_clean = ratings.drop_duplicates()

books_clean['language_code'] = books_clean['language_code'].fillna(books_clean['language_code'].mode()[0])

le_author = LabelEncoder()
le_language = LabelEncoder()

books_clean['author_encoded'] = le_author.fit_transform(books_clean['authors'])
books_clean['language_encoded'] = le_language.fit_transform(books_clean['language_code'])

scaler = MinMaxScaler()
numerical_cols = ['average_rating', 'ratings_count', 'original_publication_year']
books_clean[numerical_cols] = scaler.fit_transform(books_clean[numerical_cols])

content_features_num = books_clean[
    ['average_rating', 'ratings_count', 'original_publication_year', 'author_encoded', 'language_encoded']
]

tfidf = TfidfVectorizer(stop_words='english', max_features=500)
title_tfidf = tfidf.fit_transform(books_clean['title'].fillna(''))

numerical_sparse = csr_matrix(content_features_num.values)
feature_matrix = hstack([numerical_sparse, title_tfidf])

print(f"Content feature matrix shape: {feature_matrix.shape}")

print("Computing cosine similarity matrix...")
similarity_matrix = cosine_similarity(feature_matrix, dense_output=False)  # buat memory efisien
print(f"Similarity matrix shape: {similarity_matrix.shape}")

"""# Modelling & Results

### Content Base Filtering

Mencari dan menampilkan rekomendasi buku mirip berdasarkan judul buku input menggunakan metode content-based filtering dengan matriks kemiripan.
"""

# Fungsi untuk mendapatkan rekomendasi buku berdasarkan judul buku menggunakan content-based filtering
def get_book_recommendations(book_title, n=5, show=True):
    try:
        # Cek apakah kolom 'title' ada di dataframe
        if 'title' not in books_clean.columns:
            print("Kolom 'title' tidak ditemukan.")
            return None

        # Cari indeks buku yang judulnya sama dengan book_title
        idx_list = books_clean.index[books_clean['title'] == book_title].tolist()
        if not idx_list:
            print(f"'{book_title}' tidak ditemukan.")
            return None
        idx = idx_list[0]

        # Ambil skor kemiripan (similarity) dari matriks similarity untuk buku tersebut
        sim_scores = similarity_matrix[idx].toarray().flatten()
        sim_scores = list(enumerate(sim_scores))

        # Urutkan skor kemiripan secara descending dan ambil n teratas selain buku itu sendiri
        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)
        sim_scores = [x for x in sim_scores if x[0] != idx][:n]

        # Ambil indeks buku-buku yang direkomendasikan
        top_idx = [i for i, _ in sim_scores]

        # Ambil data buku rekomendasi berdasarkan indeks, termasuk judul dan penulis
        recommendations = books_clean.loc[top_idx, ['title', 'authors']].copy()
        # Tambahkan kolom skor kemiripan ke dataframe rekomendasi
        recommendations['Similarity_Score'] = [s for _, s in sim_scores]

        # Jika kolom lain ada di dataset, tambahkan juga ke rekomendasi
        for col in ['average_rating', 'ratings_count', 'original_publication_year']:
            if col in books_clean.columns:
                recommendations[col] = books_clean.loc[top_idx, col].values

        # Jika parameter show True, tampilkan hasil rekomendasi secara rapi
        if show:
            print(f"\nCONTENT-BASED RECOMMENDATIONS FOR BOOK: '{book_title}'")
            print("=" * 70)
            for i, (_, row) in enumerate(recommendations.iterrows(), 1):
                print(f"{i}. {row['title']} by {row['authors']} (Similarity: {row['Similarity_Score']:.3f})")
                if 'average_rating' in row:
                    print(f"   Average Rating: {row['average_rating']:.2f}")
                if 'ratings_count' in row:
                    print(f"   Ratings Count: {row['ratings_count']}")
                if 'original_publication_year' in row:
                    print(f"   Publication Year: {int(row['original_publication_year'])}")
                print("-" * 50)

        # Kembalikan dataframe rekomendasi
        return recommendations

    except Exception as e:
        # Tangani error dengan mencetak pesan error dan kembalikan None
        print(f"Error: {e}")
        return None

"""Menampilkan 10 judul buku secara acak dari dataframe books_clean dan menampilkannya dalam bentuk array."""

print(books_clean['title'].sample(10, random_state=42).values)

"""Memeriksa keberadaan sebuah judul buku dalam data, menampilkan ukuran matriks kemiripan yang digunakan, lalu memberikan dan menampilkan 5 rekomendasi buku yang paling mirip berdasarkan skor kemiripan, lengkap dengan detail seperti pengarang, rating, jumlah rating, dan tahun terbit jika tersedia."""

judul_cari = 'Scion of Ikshvaku (RamChandra, #1)'
print(f"Ada judul '{judul_cari}' di data?", judul_cari in books_clean['title'].values)

print("Shape similarity_matrix:", similarity_matrix.shape)

idx = books_clean.index[books_clean['title'] == judul_cari][0]
print("Tipe similarity_matrix[idx]:", type(similarity_matrix[idx]))

get_book_recommendations(judul_cari, n=5, show=True)

"""Mengukur seberapa baik sistem rekomendasi content-based dalam memberikan rekomendasi yang relevan berdasarkan penulis buku. Dengan mengambil sampel acak dari data buku, fungsi ini menghasilkan rekomendasi untuk setiap buku sampel dan menghitung Precision@k, yaitu proporsi rekomendasi yang memiliki penulis sama dengan buku yang dicari. Hasil evaluasi berupa rata-rata Precision@k dari seluruh sampel, yang menunjukkan tingkat akurasi sistem rekomendasi pada data tersebut."""

def evaluate_content_based_precision_at_k(k=5, sample_size=30):
    print(f"Evaluating Content-Based Filtering dengan Precision@{k}")

    if 'books_clean' not in globals():
        print("Dataframe 'books_clean' tidak ditemukan.")
        return None

    current_sample_size = min(sample_size, len(books_clean))
    if current_sample_size == 0:
        print("Tidak ada buku untuk dievaluasi.")
        return None

    sample_books = books_clean.sample(current_sample_size, random_state=42)
    precisions = []

    for _, book_row in sample_books.iterrows():
        book_title = book_row['title']
        book_author = book_row['authors']

        recommendations = get_book_recommendations(book_title, n=k, show=False)
        if recommendations is not None and not recommendations.empty:

            relevant_items = sum(recommendations['authors'] == book_author)
            precision = relevant_items / k
            precisions.append(precision)

    if precisions:
        avg_precision = np.mean(precisions)
        print(f"Average Precision@{k}: {avg_precision:.4f}")
        print(f"Evaluated on {len(precisions)} samples")
        print(f"Precision range: {min(precisions):.4f} - {max(precisions):.4f}")
        return avg_precision
    else:
        print("Could not evaluate Precision@K")
        return None

"""# EVALUATION

Untuk evaluasi pada bagian content-based dengan menggunakan Metrik Precision@
"""

cbf_precision = evaluate_content_based_precision_at_k(k=5, sample_size=30)
print(f"Precision@5 dari content-based filtering: {cbf_precision}")

